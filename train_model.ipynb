{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.regularizers import l2\n",
    "from keras.layers import TimeDistributed, Bidirectional, BatchNormalization, Dropout, Input, Add, Masking\n",
    "from keras import Model\n",
    "import pdb\n",
    "import pandas as pd\n",
    "from keras.callbacks import EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.models import load_model\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import glob\n",
    "import sys\n",
    "from sklearn.metrics import auc\n",
    "import pickle"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "BATCH_SIZE = 64\n",
    "INPUT_SEQ_LEN_MODEL1 = 10\n",
    "INPUT_NUM_CH_MODEL1 = 5\n",
    "INPUT_FEATS_MODEL2 = 33"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def roc(predictions, true, filename):\n",
    "    predictions = predictions.flatten()\n",
    "    true = true.flatten()\n",
    "\n",
    "    thresh_vals = np.linspace(np.min(predictions), np.max(predictions), 50)\n",
    "    results = []\n",
    "    for thresh in thresh_vals:\n",
    "        tmp_predictions = (predictions > thresh).astype(int)\n",
    "        tn, fp, fn, tp = confusion_matrix(true, tmp_predictions).ravel()\n",
    "        tpr = tp/(tp+fn)\n",
    "        fpr = fp/(tn+fp)\n",
    "        acc = (tp+tn)/(tn+fp+fn+tp)\n",
    "\n",
    "        tmp_dict = {'acc': acc, 'tpr': tpr, 'fpr': fpr, 'thresh': thresh}\n",
    "        results.append(tmp_dict)\n",
    "\n",
    "    results = pd.DataFrame(results)\n",
    "    results = results.sort_values(by='thresh', ascending=False)\n",
    "    \n",
    "    #calculate the AUC\n",
    "    AUC = auc(results['fpr'].values, results['tpr'].values)\n",
    "\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.plot(results['fpr'], results['tpr'], '*-')\n",
    "    plt.xlabel('FPR')\n",
    "    plt.ylabel('TPR')\n",
    "    plt.title('ROC\\nAUC=%.2f' % AUC)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('plots/ROC_%s.png' % filename, dpi=250)\n",
    "    plt.show()\n",
    "\n",
    "    results = results.sort_values(by='acc', ascending=False)\n",
    "    final_thresh = results.head(1)['thresh'].values[0]\n",
    "\n",
    "    return results, final_thresh"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#load the training data\n",
    "train = pd.read_pickle('train.pkl').reset_index(drop=True)\n",
    "\n",
    "#define the validation set as 15% of the training set\n",
    "N = len(train)\n",
    "train = train.sample(frac=1).reset_index(drop=True)\n",
    "val = train[:int(N*0.2)]\n",
    "train = train[int(N*0.2):]\n",
    "\n",
    "#separate out the training data\n",
    "#note that input data for LSTM should be in the format of (number_samples, sequence_len, num_channels)\n",
    "y_train = np.asarray(list(train['label']))\n",
    "y_train = to_categorical(y_train)\n",
    "X_train_cont = np.asarray(list(train['X_cont'].values))\n",
    "X_train_cat = train.drop(['X_cont', 'label'], axis=1)\n",
    "\n",
    "#for the mask layer, any NaN values need to be replaced by a unique value. use the constant np.pi\n",
    "X_train_cat[X_train_cat.isna()] = np.pi\n",
    "X_train_cat = np.asarray(list(X_train_cat.values))\n",
    "\n",
    "#separate out the training data\n",
    "y_val = np.asarray(list(val['label']))\n",
    "y_val = to_categorical(y_val)\n",
    "X_val_cont = np.asarray(list(val['X_cont'].values))\n",
    "X_val_cat = val.drop(['X_cont', 'label'], axis=1)\n",
    "\n",
    "#replace NaN with pi\n",
    "X_val_cat[X_val_cat.isna()] = np.pi\n",
    "X_val_cat = np.asarray(list(X_val_cat.values))\n",
    "\n",
    "#load the test data\n",
    "test = pd.read_pickle('test.pkl')\n",
    "y_test = np.asarray(list(test['label']))\n",
    "y_test = to_categorical(y_test)\n",
    "X_test_cont = np.asarray(list(test['X_cont'].values))\n",
    "X_test_cat = test.drop(['X_cont', 'label'], axis=1)\n",
    "\n",
    "#replace NaN with pi\n",
    "X_test_cat[X_test_cat.isna()] = np.pi\n",
    "X_test_cat = np.asarray(list(X_test_cat.values))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "count_class_0 = np.sum(y_train[:,1].astype(int)==0)\n",
    "count_class_1 = np.sum(y_train[:, 1].astype(int) == 1)\n",
    "max_class_counts = np.max((count_class_0, count_class_1))\n",
    "class_weights = {0: max_class_counts/count_class_0, 1: max_class_counts/count_class_1}\n",
    "print('class weights -- no sepsis: {}, sepsis: {}'.format(class_weights[0], class_weights[1]))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "input1 = Input(shape=(INPUT_SEQ_LEN_MODEL1, INPUT_NUM_CH_MODEL1))\n",
    "model1 = Bidirectional(LSTM(100, kernel_regularizer=l2(0.001), return_sequences=True))(input1)\n",
    "model1 = Bidirectional(LSTM(75, kernel_regularizer=l2(0.001)))(model1)\n",
    "model1 = Dense(35, kernel_regularizer=l2(0.001), activation='relu')(model1)\n",
    "model1 = BatchNormalization()(model1)\n",
    "model1 = Dense(15, kernel_regularizer=l2(0.001), activation='relu')(model1)\n",
    "model1 = BatchNormalization()(model1)\n",
    "\n",
    "input2 = Input(shape=(INPUT_FEATS_MODEL2,))\n",
    "model2 = Masking(mask_value=np.pi)(input2)\n",
    "model2 = Dense(30, kernel_regularizer=l2(0.001), activation='relu')(model2)\n",
    "model2 = BatchNormalization()(model2)\n",
    "model2 = Dense(15, kernel_regularizer=l2(0.001), activation='relu')(model2)\n",
    "model2 = BatchNormalization()(model2)\n",
    "\n",
    "model_add = Add()([model1, model2])\n",
    "output = Dense(2, kernel_regularizer=l2(0.001), activation='softmax')(model_add)\n",
    "\n",
    "model = Model(inputs=[input1, input2], outputs=output)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "print(model.summary())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "checkpoint = ModelCheckpoint('model.h5', monitor='val_loss', verbose=1, save_best_only=True, mode='auto', period=1)\n",
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=5)\n",
    "history = model.fit([X_train_cont, X_train_cat],\n",
    "                    y_train,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    epochs=50,\n",
    "                    validation_data=([X_val_cont, X_val_cat], y_val),\n",
    "                    callbacks=[earlystop, checkpoint],\n",
    "                    class_weight=class_weights,\n",
    "                    verbose=1)\n",
    "\n",
    "#save the history\n",
    "pickle.dump(history, open('history.pkl', 'wb'))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Plot training & validation loss values\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(history.history['loss'], '*-')\n",
    "plt.plot(history.history['val_loss'], '*-')\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.savefig('plots/loss.png', dpi=250)\n",
    "plt.show()\n",
    "plt.close()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "predictions = model.predict([X_val_cont, X_val_cat])\n",
    "\n",
    "results_df, thresh_final = roc(predictions[:,1].flatten(), y_val[:,1].flatten(), 'val')\n",
    "results_df = results_df.sort_values(by='fpr')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "predictions = model.predict([X_test_cont, X_test_cat])\n",
    "\n",
    "results_df, thresh_final = roc(predictions[:,1].flatten(), y_test[:,1].flatten(), 'test')\n",
    "results_df = results_df.sort_values(by='fpr')"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}